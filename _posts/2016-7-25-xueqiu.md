---
layout: post
title: Xueqiu Social network analysis
image: http://www.urbansplatter.com/wp-content/uploads/2014/06/91326403.jpg
color: '#949667'
tags: [Trading]
---
# Xueqiu social network analysis
Xueqiu (xueqiu.com) is a popular investing social network in china, where people share their invetsting suggestions, news and other information. The most interesting function is its virtual investing portfolio where everyone could build and operate, then share the pnl wth others. //
The first goal of this python script, is to build a workflow of scraping operation records from those virtual portfolios, then see whether the operations in virtual portfolios are profitable operations. After that, I will try to find some users whose operations are mostly profitable, and then use their portfolio as a reference in investing.

```python
import re
import requests
from bs4 import BeautifulSoup
import json
import urllib.request
import http.cookiejar
from time import sleep
import pprint
from pandas.tseries.offsets import *
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tushare as ts
import datetime
pp = pprint.PrettyPrinter(indent=4) 
#SZ002024
#basic information
stockid="SZ002024"
xqprefix="http://xueqiu.com/"

#scraping followers info,get followers#, maximum page of followers
url="http://xueqiu.com/S/"+stockid+"/follows?page=1"
page=requests.get(url,timeout=5, headers={'User-Agent': 'Chrome/50.0.2661.94'})
html=page.content
soup=BeautifulSoup(page.text,'html5lib')
stockinfo=soup.find_all("div",class_='stockInfo')
numfollower=int(stockinfo[0].find_all("span")[0].text[4:-2])
maxfollowpage=int(numfollower/20)-1
print(maxfollowpage)

#get follower's id, which brings us to the personal pages
userids=[]
def get_follower_id(url):
    page=requests.get(url,timeout=5, headers={'User-Agent': 'Chrome/50.0.2661.94'})
    soup=BeautifulSoup(page.text,'html5lib')
    #users = soup.find_all("follower-list") 
    main = soup.find_all("div",class_='main') 
    a=main[0].find_all("script")
    data=a[0].string
    expression=re.compile('follows=(.*?)};')
    userjson=expression.search(data)
    users = json.loads(userjson.groups()[0]+"}")
    for user in users["followers"]:
         if "手机用户" not in user['screen_name']:
             userids.append(str(user['id']))     
             print (user['id'])
             print (user['screen_name'])
    return
follower_prefix="http://xueqiu.com/S/"+stockid+"/follows?page="
#get_follower_id("http://xueqiu.com/S/"+stockid+"/follows?page=12")
#user pages searched:1500
for i in range(1500,4000):
    follower_url=follower_prefix+str(i)
    sleep(1)
    try:
        get_follower_id(follower_url)
    except Exception:
        pass
    print(str(i*20)+" users catched!")
get_follower_id("http://xueqiu.com/S/SZ002024/follows?page=537")
len(userids)
fo = open("userids.txt", "a+")
fo.write(str(userids))
fo.close()
fo = open("userids.txt", "r")
userids=fo.read()
fo.close()
userids
userids=userids[1:-1].split(",")
print(userids[2][2:-1])

#Initialize, get cookies
#设置cookie
CookieJar = http.cookiejar.CookieJar()
CookieProcessor = urllib.request.HTTPCookieProcessor(CookieJar)
opener = urllib.request.build_opener(CookieProcessor)
urllib.request.install_opener(opener)
#登陆获得cookie
params = urllib.parse.urlencode({'username':'pyc1995@126.com','password':'price2013'}).encode(encoding='UTF8')
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0'}
request = urllib.request.Request('http://xueqiu.com/user/login',headers=headers)
httpf = opener.open(request, params)

#get portfolio ids, which brings us to the portfolio page
portids=[]
def get_port_id(cubeurl):

    req = urllib.request.Request(cubeurl,headers=headers)
    html = urllib.request.urlopen(req).read().decode('utf-8')
    portdata = json.loads(html)
    if portdata['list']!=[]:
        num_port=len(portdata['list'])
        for i in range(num_port):
           portids.append(portdata['list'][i]['symbol'])
           print(portdata['list'][i]['symbol'])
    return
cubeurl_prefix = 'http://xueqiu.com/cubes/list.json?user_id=' 
#number of users searched:36000
for i in range(25000,36000):
            sleep(1)   
            try:
                get_port_id(cubeurl_prefix+str(userids[i][2:-1]))
            except Exception:
                pass
            print(str(i+1)+" users checked!")
len(portids)
portids_towrite=str(portids)[1:-1]+", "
fo = open("portids.txt", "a+")
fo.write(portids_towrite)
fo.close()
portids_towrite
fo = open("portids.txt", "r")
portids=fo.read()
fo.close()
portids=portids[1:-1].split(",")
portids=portids[0:-1]
print(portids[-1][2:-1])
len(portids)

#get portfolio's current position
portfolios={}
def get_port(portid):
    porturl_prefix='http://xueqiu.com/P/'
    porturl=porturl_prefix+portid
    portpage=requests.get(porturl,timeout=5, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11'})
    #print(portpage.text)
    portsoup=BeautifulSoup(portpage.content,'lxml')
    weightlist=portsoup.find_all("div",class_="weight-list")
    stocks=[]
    weights=[]
    stock=weightlist[0].find_all("a",class_="stock fn-clear no-tooltip")
    #print(len(stock))
    #print(stock[0].prettify())
    portfolio={}
    weights={}
    for i in range(len(stock)):
        name=stock[i].find_all("div",class_="price")
        weight=stock[i].find_all("span",class_="stock-weight weight")
        stocks.append(name[0].text)
        weights[name[0].text]=weight[0].text
        
    portfolio={'stocks':stocks,'weights':weights}
    portfolios[portid]=portfolio
    return
get_port("ZH060968")
portfolios
for i in range(1,161):
    sleep(1)
    try:
        get_port(portids[i][2:-1])
    except Exception:
        pass
    print(str(i+1)+" portfolios synchronized!")

#extract portfolios within target stocks    
target_port={}
for i in portfolios.keys():
    if stockid in portfolios[i]['stocks']:    
        target_port[i]=portfolios[i]        
        print(portfolios[i]['weights'][stockid])
pp.pprint(target_port)

#search rebalance history for the transcation of target stock
target_ports_h=[]
def search_history(port_id):
    success=False
    c_weight=0
    p_weight=0
    n_time=0
    rb_hist_url="http://xueqiu.com/cubes/rebalancing/history.json?cube_symbol="+port_id+"&count=20&page=1"
    rb_hist_req = urllib.request.Request(rb_hist_url,headers=headers)
    rb_hist_html = urllib.request.urlopen(rb_hist_req).read().decode('utf-8')
    rb_hist_data = json.loads(rb_hist_html)
    #print(rb_hist_data)
    #print(rb_hist_data['list'][0]['rebalancing_histories'][0]['stock_symbol'])
    rb_count=len(rb_hist_data['list'])
    result_list=[]
    for i in range(rb_count):
        stock_count=len(rb_hist_data['list'][i]['rebalancing_histories'])
        if rb_hist_data['list'][i]['category'] != "sys_rebalancing":
            for j in range(stock_count):
                if (rb_hist_data['list'][i]['rebalancing_histories'][j]['stock_symbol']) == stockid:
                    #print(port_id)
                    p_weight=rb_hist_data['list'][i]['rebalancing_histories'][j]['prev_weight_adjusted']
                    if p_weight is None:
                        p_weight=0
                    c_weight=rb_hist_data['list'][i]['rebalancing_histories'][j]['weight']
                    delta_weight=c_weight-p_weight
                    e_stamp=rb_hist_data['list'][i]['rebalancing_histories'][j]['updated_at']
                    o_time=pd.to_datetime(int(e_stamp), unit='ms')
                    n_time=pd.to_datetime(str(o_time)[0:14]+"00:00")
                    print(n_time)
                    n_time=n_time+DateOffset(hours=9)
                    print(n_time)                    
                    result_list.append((delta_weight,n_time))
                    success=True
                    
    return [success,result_list]
search_history(portids[1])[0]

#construct database
timeindex=pd.date_range('2014-10-1', periods=20000, freq='H')
timeindex
db = pd.DataFrame(data=np.zeros(len(timeindex)), index=pd.DatetimeIndex(timeindex))
db.columns=['pos_change']

#read database
db=pd.read_csv('db.csv',header=0)
db['Unnamed: 0']=pd.to_datetime(db['Unnamed: 0'])+DateOffset(hours=17)            
(db['Unnamed: 0'][19999].hour)
sum(db['pos_change'])
#0000
for i in range(1,1520):
    sleep(1)
    try:
        #if search_history(portids[i][2:-1])[0]:
            #target_ports_h.append(portids[i][2:-1])
            #print(portids[i][2:-1])
        
            #results=search_history(portids[i][2:-1])[1])
        if search_history(portids[i])[0]:
            target_ports_h.append(portids[i])
            print(portids[i])
        
            results=search_history(portids[i])[1]
            for j in range(len(results)):
                t_index=results[j][1]
                db['pos_change'][t_index]=db['pos_change'][t_index]+results[j][0]
                print("Time: "+str(t_index))
    except Exception:
        pass
    print(str(i+1)+" portfolios searched!")
    
#write database
db.to_csv('db.csv')
db
plt.plot(db['Unnamed: 0'],db['pos_change'])

# write target_ports_h
len(target_ports_h)
ports_towrite=str(target_ports_h)[1:-1]+", "
ports_towrite
fo = open("target_ports_h.txt", "a+")
fo.write(ports_towrite)
fo.close()
fo = open("target_ports_h.txt", "r")
portids=fo.read()
fo.close()
portids=portids[0:-1]
portids=portids[:-1].split(",")
portids=portids[1:-1]
portids[0]
for i in range(len(portids)):
   portids[i]=portids[i][2:-1]
portids[3]
len(portids)
# # of ports searched: 6650    

#construct database based on date
dayindex=pd.date_range('2014-10-1', periods=835, freq='D')
dayindex
day_db = pd.DataFrame(data=np.zeros(len(dayindex)), index=pd.DatetimeIndex(dayindex))
day_db.columns=['pos_change']
day_db
for i in range(len(db)):
    if db['Unnamed: 0'][i].hour <9:
        date=pd.to_datetime(str(db['Unnamed: 0'][i])[:10])
        day_db['pos_change'][date]+=db['pos_change'][i]
    elif db['Unnamed: 0'][i].hour >15:
        date=pd.to_datetime(str(db['Unnamed: 0'][i])[:10])+DateOffset(days=1)
        day_db['pos_change'][date]+=db['pos_change'][i]
        
#retrieve stock historical data
start=str(day_db.index[0])[:10]
end=str(day_db.index[-1])[:10]
prices=ts.get_hist_data('002024',start,end)

#construct final databease by combining daybase and historical prices
finindex=pd.date_range(prices.index[-1], prices.index[0], freq='D')
fin_data={'prices':prices.close,'pos_change':prices.close}
fin_db=pd.DataFrame(data=fin_data,index=prices.index)
for i in (fin_db.index):
    fin_db['pos_change'][i]=day_db['pos_change'][pd.to_datetime(i[:10])]

#write final database
fin_db.to_csv('fin_db.csv')
```